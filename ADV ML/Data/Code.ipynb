{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "914430e9-1db4-4a56-9812-3cb1e22f0385",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jaya Prakash\\Desktop\\ADV ML\\Data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5706c6f0-a457-4043-943b-6b7b46eeca4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cb9ba09-33a9-43bb-a661-83a1a37f6060",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./NER_Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99bfbcbc-ac00-48da-9bc0-b012b40c6a5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_ID</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>47959</td>\n",
       "      <td>47959</td>\n",
       "      <td>47959</td>\n",
       "      <td>47959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>47959</td>\n",
       "      <td>47575</td>\n",
       "      <td>47214</td>\n",
       "      <td>33318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>['VOA', \"'s\", 'Mil', 'Arcega', 'reports', '.']</td>\n",
       "      <td>['NNP', 'POS', 'NNP', 'NNP', 'VBZ', '.']</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>39</td>\n",
       "      <td>450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Sentence_ID                                            Word  \\\n",
       "count         47959                                           47959   \n",
       "unique        47959                                           47575   \n",
       "top     Sentence: 1  ['VOA', \"'s\", 'Mil', 'Arcega', 'reports', '.']   \n",
       "freq              1                                              17   \n",
       "\n",
       "                                             POS  \\\n",
       "count                                      47959   \n",
       "unique                                     47214   \n",
       "top     ['NNP', 'POS', 'NNP', 'NNP', 'VBZ', '.']   \n",
       "freq                                          39   \n",
       "\n",
       "                                                      Tag  \n",
       "count                                               47959  \n",
       "unique                                              33318  \n",
       "top     ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "freq                                                  450  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ce5cbf0-4db3-433a-a859-92b7cdc79b2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Sentence_ID', 'Word', 'POS', 'Tag'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4c0f185-1eca-471b-9627-ca3323f625c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 47959 entries, 0 to 47958\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Sentence_ID  47959 non-null  object\n",
      " 1   Word         47959 non-null  object\n",
      " 2   POS          47959 non-null  object\n",
      " 3   Tag          47959 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cec544e2-d9bd-4f5b-8344-fd0028db31c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_ID</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>['Thousands', 'of', 'demonstrators', 'have', '...</td>\n",
       "      <td>['NNS', 'IN', 'NNS', 'VBP', 'VBN', 'IN', 'NNP'...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 10</td>\n",
       "      <td>['Iranian', 'officials', 'say', 'they', 'expec...</td>\n",
       "      <td>['JJ', 'NNS', 'VBP', 'PRP', 'VBP', 'TO', 'VB',...</td>\n",
       "      <td>['B-gpe', 'O', 'O', 'O', 'O', 'O', 'O', 'O', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 100</td>\n",
       "      <td>['Helicopter', 'gunships', 'Saturday', 'pounde...</td>\n",
       "      <td>['NN', 'NNS', 'NNP', 'VBD', 'JJ', 'NNS', 'IN',...</td>\n",
       "      <td>['O', 'O', 'B-tim', 'O', 'O', 'O', 'O', 'O', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1000</td>\n",
       "      <td>['They', 'left', 'after', 'a', 'tense', 'hour-...</td>\n",
       "      <td>['PRP', 'VBD', 'IN', 'DT', 'NN', 'JJ', 'NN', '...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 10000</td>\n",
       "      <td>['U.N.', 'relief', 'coordinator', 'Jan', 'Egel...</td>\n",
       "      <td>['NNP', 'NN', 'NN', 'NNP', 'NNP', 'VBD', 'NNP'...</td>\n",
       "      <td>['B-geo', 'O', 'O', 'B-per', 'I-per', 'O', 'B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sentence: 10001</td>\n",
       "      <td>['Mr.', 'Egeland', 'said', 'the', 'latest', 'f...</td>\n",
       "      <td>['NNP', 'NNP', 'VBD', 'DT', 'JJS', 'NNS', 'VBP...</td>\n",
       "      <td>['B-per', 'I-per', 'O', 'O', 'O', 'O', 'O', 'O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sentence: 10002</td>\n",
       "      <td>['He', 'said', 'last', 'week', \"'s\", 'tsunami'...</td>\n",
       "      <td>['PRP', 'VBD', 'JJ', 'NN', 'POS', 'NN', 'CC', ...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sentence: 10003</td>\n",
       "      <td>['Some', '1,27,000', 'people', 'are', 'known',...</td>\n",
       "      <td>['DT', 'CD', 'NNS', 'VBP', 'VBN', 'JJ', '.']</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sentence: 10004</td>\n",
       "      <td>['Aid', 'is', 'being', 'rushed', 'to', 'the', ...</td>\n",
       "      <td>['NNP', 'VBZ', 'VBG', 'VBN', 'TO', 'DT', 'NN',...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sentence: 10005</td>\n",
       "      <td>['Lebanese', 'politicians', 'are', 'condemning...</td>\n",
       "      <td>['JJ', 'NNS', 'VBP', 'VBG', 'NNP', 'POS', 'NN'...</td>\n",
       "      <td>['B-gpe', 'O', 'O', 'O', 'B-tim', 'O', 'O', 'O...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sentence_ID                                               Word  \\\n",
       "0      Sentence: 1  ['Thousands', 'of', 'demonstrators', 'have', '...   \n",
       "1     Sentence: 10  ['Iranian', 'officials', 'say', 'they', 'expec...   \n",
       "2    Sentence: 100  ['Helicopter', 'gunships', 'Saturday', 'pounde...   \n",
       "3   Sentence: 1000  ['They', 'left', 'after', 'a', 'tense', 'hour-...   \n",
       "4  Sentence: 10000  ['U.N.', 'relief', 'coordinator', 'Jan', 'Egel...   \n",
       "5  Sentence: 10001  ['Mr.', 'Egeland', 'said', 'the', 'latest', 'f...   \n",
       "6  Sentence: 10002  ['He', 'said', 'last', 'week', \"'s\", 'tsunami'...   \n",
       "7  Sentence: 10003  ['Some', '1,27,000', 'people', 'are', 'known',...   \n",
       "8  Sentence: 10004  ['Aid', 'is', 'being', 'rushed', 'to', 'the', ...   \n",
       "9  Sentence: 10005  ['Lebanese', 'politicians', 'are', 'condemning...   \n",
       "\n",
       "                                                 POS  \\\n",
       "0  ['NNS', 'IN', 'NNS', 'VBP', 'VBN', 'IN', 'NNP'...   \n",
       "1  ['JJ', 'NNS', 'VBP', 'PRP', 'VBP', 'TO', 'VB',...   \n",
       "2  ['NN', 'NNS', 'NNP', 'VBD', 'JJ', 'NNS', 'IN',...   \n",
       "3  ['PRP', 'VBD', 'IN', 'DT', 'NN', 'JJ', 'NN', '...   \n",
       "4  ['NNP', 'NN', 'NN', 'NNP', 'NNP', 'VBD', 'NNP'...   \n",
       "5  ['NNP', 'NNP', 'VBD', 'DT', 'JJS', 'NNS', 'VBP...   \n",
       "6  ['PRP', 'VBD', 'JJ', 'NN', 'POS', 'NN', 'CC', ...   \n",
       "7       ['DT', 'CD', 'NNS', 'VBP', 'VBN', 'JJ', '.']   \n",
       "8  ['NNP', 'VBZ', 'VBG', 'VBN', 'TO', 'DT', 'NN',...   \n",
       "9  ['JJ', 'NNS', 'VBP', 'VBG', 'NNP', 'POS', 'NN'...   \n",
       "\n",
       "                                                 Tag  \n",
       "0  ['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', '...  \n",
       "1  ['B-gpe', 'O', 'O', 'O', 'O', 'O', 'O', 'O', '...  \n",
       "2  ['O', 'O', 'B-tim', 'O', 'O', 'O', 'O', 'O', '...  \n",
       "3  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "4  ['B-geo', 'O', 'O', 'B-per', 'I-per', 'O', 'B-...  \n",
       "5  ['B-per', 'I-per', 'O', 'O', 'O', 'O', 'O', 'O...  \n",
       "6  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "7                ['O', 'O', 'O', 'O', 'O', 'O', 'O']  \n",
       "8  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "9  ['B-gpe', 'O', 'O', 'O', 'B-tim', 'O', 'O', 'O...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "947903c7-b2c9-4f78-9d81-8eab8b9fc97e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "def get_dict_map(data, token_or_tag):\n",
    "    tok2idx = {}\n",
    "    idx2tok = {}\n",
    "    \n",
    "    if token_or_tag == 'token':\n",
    "        vocab = list(set(data['Word'].to_list()))\n",
    "    else:\n",
    "        vocab = list(set(data['Tag'].to_list()))\n",
    "    \n",
    "    idx2tok = {idx:tok for  idx, tok in enumerate(vocab)}\n",
    "    tok2idx = {tok:idx for  idx, tok in enumerate(vocab)}\n",
    "    return tok2idx, idx2tok\n",
    "token2idx, idx2token = get_dict_map(data, 'token')\n",
    "tag2idx, idx2tag = get_dict_map(data, 'tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18ef4381-9d78-44b4-83a2-7b011d01b720",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['Word_idx'] = data['Word'].map(token2idx)\n",
    "data['Tag_idx'] = data['Tag'].map(tag2idx)\n",
    "data_fillna = data.ffill(axis=0)  # Forward fill missing values\n",
    "\n",
    "# Groupby and collect columns\n",
    "data_group = data_fillna.groupby(\n",
    "['Sentence_ID'],as_index=False\n",
    ")[['Word', 'POS', 'Tag', 'Word_idx', 'Tag_idx']].agg(lambda x: list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3d5c54c-822e-42d7-baa1-abf61a327c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Jaya Prakash\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "train_tokens length: 32372 \n",
      "train_tokens length: 32372 \n",
      "test_tokens length: 4796 \n",
      "test_tags: 4796 \n",
      "val_tokens: 10791 \n",
      "val_tags: 10791\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def get_pad_train_test_val(data_group, data):\n",
    "\n",
    "    #get max token and tag length\n",
    "    n_token = len(list(set(data['Word'].to_list())))\n",
    "    n_tag = len(list(set(data['Tag'].to_list())))\n",
    "\n",
    "    #Pad tokens (X var)    \n",
    "    tokens = data_group['Word_idx'].tolist()\n",
    "    maxlen = max([len(s) for s in tokens])\n",
    "    pad_tokens = pad_sequences(tokens, maxlen=maxlen, dtype='int32', padding='post', value= n_token - 1)\n",
    "    \n",
    "    #Pad Tags (y var) and convert it into one hot encoding\n",
    "    tag2idx['O'] = len(tag2idx)  # Assign the next available index value to 'O'\n",
    "    tags = data_group['Tag_idx'].tolist()\n",
    "    pad_tags = pad_sequences(tags, maxlen=maxlen, dtype='int32', padding='post', value= tag2idx[\"O\"])\n",
    "    n_tags = len(tag2idx)\n",
    "    pad_tags = [to_categorical(i, num_classes=n_tags) for i in pad_tags]\n",
    "    \n",
    "    #Split train, test and validation set\n",
    "    tokens_, test_tokens, tags_, test_tags = train_test_split(pad_tokens, pad_tags, test_size=0.1, train_size=0.9, random_state=2020)\n",
    "    train_tokens, val_tokens, train_tags, val_tags = train_test_split(tokens_,tags_,test_size = 0.25,train_size =0.75, random_state=2020)\n",
    "\n",
    "    print(\n",
    "        'train_tokens length:', len(train_tokens),\n",
    "        '\\ntrain_tokens length:', len(train_tokens),\n",
    "        '\\ntest_tokens length:', len(test_tokens),\n",
    "        '\\ntest_tags:', len(test_tags),\n",
    "        '\\nval_tokens:', len(val_tokens),\n",
    "        '\\nval_tags:', len(val_tags),\n",
    "    )\n",
    "    \n",
    "    return train_tokens, val_tokens, test_tokens, train_tags, val_tags, test_tags\n",
    "\n",
    "train_tokens, val_tokens, test_tokens, train_tags, val_tags, test_tags = get_pad_train_test_val(data_group, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32c844b6-546b-4e0a-a978-9af7c47c9c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow\n",
    "from tensorflow.keras import Sequential, Model, Input\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "tensorflow.random.set_seed(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cffdfaa-8a85-4848-93ec-499b9bcfe380",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(list(set(data['Word'].to_list())))+1\n",
    "output_dim = 64\n",
    "input_length = max([len(s) for s in data_group['Word_idx'].tolist()])\n",
    "n_tags = len(tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "693764b5-ee06-4078-92fd-b5ca4568212c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bilstm_lstm_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add Embedding layer\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length))\n",
    "\n",
    "    # Add bidirectional LSTM\n",
    "    model.add(Bidirectional(LSTM(units=output_dim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode = 'concat'))\n",
    "\n",
    "    # Add LSTM\n",
    "    model.add(LSTM(units=output_dim, return_sequences=True, dropout=0.5, recurrent_dropout=0.5))\n",
    "\n",
    "    # Add timeDistributed Layer\n",
    "    model.add(TimeDistributed(Dense(n_tags, activation=\"relu\")))\n",
    "\n",
    "    #Optimiser \n",
    "    # adam = k.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "def train_model(X, y, model):\n",
    "    loss = list()\n",
    "    for i in range(25):\n",
    "        # fit model for one epoch on this sequence\n",
    "        hist = model.fit(X, y, batch_size=1000, verbose=1, epochs=1, validation_split=0.2)\n",
    "        loss.append(hist.history['loss'][0])\n",
    "    return loss\n",
    "from keras_visualizer import visualizer\n",
    "model_bilstm_lstm = get_bilstm_lstm_model()\n",
    "visualizer(model_bilstm_lstm)\n",
    "results = pd.DataFrame()\n",
    "results['with_add_lstm'] = train_model(train_tokens, np.array(train_tags), model_bilstm_lstm)\n",
    "model_bilstm_lstm.save('model_bilstm_lstm.h5')  # Saves the model to a HDF5 file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a6fb325-e179-40c2-8f0d-9e9e64e2eae2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5638fbe-235a-428b-ad19-a4aeda91f6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 1, 64)             3044864   \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirecti  (None, 1, 128)            66048     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 1, 64)             49408     \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDi  (None, 1, 33319)          2165735   \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5326055 (20.32 MB)\n",
      "Trainable params: 5326055 (20.32 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "26/26 [==============================] - 29s 563ms/step - loss: 14.6098 - accuracy: 0.0051 - val_loss: 14.6705 - val_accuracy: 0.0037\n",
      "26/26 [==============================] - 13s 501ms/step - loss: 14.4533 - accuracy: 0.0055 - val_loss: 14.6234 - val_accuracy: 0.0037\n",
      "26/26 [==============================] - 14s 520ms/step - loss: 14.3961 - accuracy: 0.0055 - val_loss: 14.5605 - val_accuracy: 0.0037\n",
      "26/26 [==============================] - 15s 556ms/step - loss: 14.3487 - accuracy: 0.0055 - val_loss: 14.5047 - val_accuracy: 0.0037\n",
      "26/26 [==============================] - 16s 597ms/step - loss: 14.3362 - accuracy: 0.0055 - val_loss: 14.4745 - val_accuracy: 0.0037\n",
      "26/26 [==============================] - 16s 599ms/step - loss: 14.3228 - accuracy: 0.0103 - val_loss: 14.4563 - val_accuracy: 0.0076\n",
      "26/26 [==============================] - 19s 697ms/step - loss: 14.2945 - accuracy: 0.0105 - val_loss: 14.4436 - val_accuracy: 0.0077\n",
      "26/26 [==============================] - 13s 494ms/step - loss: 14.2658 - accuracy: 0.0104 - val_loss: 14.4443 - val_accuracy: 0.0071\n",
      "26/26 [==============================] - 13s 492ms/step - loss: 14.2434 - accuracy: 0.0104 - val_loss: 14.4527 - val_accuracy: 0.0074\n",
      "26/26 [==============================] - 13s 497ms/step - loss: 14.2245 - accuracy: 0.0104 - val_loss: 14.4657 - val_accuracy: 0.0074\n",
      "26/26 [==============================] - 16s 629ms/step - loss: 14.2071 - accuracy: 0.0104 - val_loss: 14.4845 - val_accuracy: 0.0071\n",
      "26/26 [==============================] - 13s 492ms/step - loss: 14.1897 - accuracy: 0.0104 - val_loss: 14.4793 - val_accuracy: 0.0071\n",
      "26/26 [==============================] - 13s 505ms/step - loss: 14.1783 - accuracy: 0.0104 - val_loss: 14.4818 - val_accuracy: 0.0066\n",
      "26/26 [==============================] - 13s 498ms/step - loss: 14.1670 - accuracy: 0.0105 - val_loss: 14.4939 - val_accuracy: 0.0059\n",
      "26/26 [==============================] - 13s 514ms/step - loss: 14.1551 - accuracy: 0.0120 - val_loss: 14.4992 - val_accuracy: 0.0053\n",
      "26/26 [==============================] - 15s 574ms/step - loss: 14.1371 - accuracy: 0.0160 - val_loss: 14.5041 - val_accuracy: 0.0040\n",
      "26/26 [==============================] - 13s 499ms/step - loss: 14.1144 - accuracy: 0.0273 - val_loss: 14.5003 - val_accuracy: 0.0043\n",
      "26/26 [==============================] - 13s 489ms/step - loss: 14.0906 - accuracy: 0.0366 - val_loss: 14.5265 - val_accuracy: 0.0049\n",
      "26/26 [==============================] - 13s 488ms/step - loss: 14.0428 - accuracy: 0.0457 - val_loss: 14.5541 - val_accuracy: 0.0059\n",
      "26/26 [==============================] - 16s 605ms/step - loss: nan - accuracy: 0.0401 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "26/26 [==============================] - 19s 728ms/step - loss: nan - accuracy: 3.8615e-05 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "26/26 [==============================] - 13s 502ms/step - loss: nan - accuracy: 3.8615e-05 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "26/26 [==============================] - 13s 500ms/step - loss: nan - accuracy: 3.8615e-05 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "26/26 [==============================] - 13s 507ms/step - loss: nan - accuracy: 3.8615e-05 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "26/26 [==============================] - 26s 1s/step - loss: nan - accuracy: 3.8615e-05 - val_loss: nan - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jaya Prakash\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "92718570-0e49-4065-91eb-f9226302d197",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Inputs to a layer should be tensors. Got 'Hi, My name is Aman Kharwal \n I am from India \n I want to work with Google \n Steve Jobs is My Inspiration' (of type <class 'str'>) as input for layer 'sequential_2'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load the model\u001b[39;00m\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m load_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_bilstm_lstm.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHi, My name is Aman Kharwal \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m I am from India \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m I want to work with Google \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m Steve Jobs is My Inspiration\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m displacy\u001b[38;5;241m.\u001b[39mrender(text, style \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ment\u001b[39m\u001b[38;5;124m'\u001b[39m, jupyter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\input_spec.py:213\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m inputs:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;66;03m# Having a shape/dtype is the only commonality of the various\u001b[39;00m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;66;03m# tensor-like objects that may be passed. The most common kind of\u001b[39;00m\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;66;03m# invalid type we are guarding for is a Layer instance (Functional API),\u001b[39;00m\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;66;03m# which does not have a `shape` attribute.\u001b[39;00m\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    214\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInputs to a layer should be tensors. Got \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    215\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(x)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) as input for layer \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    216\u001b[0m         )\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inputs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(input_spec):\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLayer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expects \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(input_spec)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m input(s),\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but it received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(inputs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m input tensors. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInputs received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minputs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    223\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: Inputs to a layer should be tensors. Got 'Hi, My name is Aman Kharwal \n I am from India \n I want to work with Google \n Steve Jobs is My Inspiration' (of type <class 'str'>) as input for layer 'sequential_2'."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from tensorflow.keras.models import load_model\n",
    "# Load the model\n",
    "model = load_model('model_bilstm_lstm.h5')\n",
    "text = model('Hi, My name is Aman Kharwal \\n I am from India \\n I want to work with Google \\n Steve Jobs is My Inspiration')\n",
    "displacy.render(text, style = 'ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d18241d-f036-4aea-a87b-127f21337723",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
